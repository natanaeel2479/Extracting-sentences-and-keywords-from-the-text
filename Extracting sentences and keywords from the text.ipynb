{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import svd\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from string import punctuation\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def read_text(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = file.read()\n",
    "    return data\n",
    "\n",
    "def tokenize_and_stem(text):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english') + list(punctuation))\n",
    "    ps = PorterStemmer()\n",
    "\n",
    "    sentences_tokens = []\n",
    "    all_tokens = set()\n",
    "\n",
    "    for sentence in sentences:\n",
    "        word_tokens = word_tokenize(sentence)\n",
    "        filtered_sentence = [ps.stem(w.lower()) for w in word_tokens if (not w.lower() in stop_words and w.isalpha())]\n",
    "        sentences_tokens.append(filtered_sentence)\n",
    "        all_tokens.update(set(filtered_sentence))\n",
    "\n",
    "    return sentences_tokens, all_tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'C:/Users/Asus/Desktop/text.txt'\n",
    "text = read_text(file_path)\n",
    "sentences_tokens, all_tokens = tokenize_and_stem(text)\n",
    "\n",
    "# # Print the result\n",
    "# for sentence_tokens in sentences_tokens:\n",
    "#     print(sentence_tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens = sorted(list(all_tokens))\n",
    "words_data = np.zeros((len(all_tokens), len(sentences_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(683, 166)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Fill the matrix\n",
    "for i, token in enumerate(all_tokens):\n",
    "    for j, sentence_tokens in enumerate(sentences_tokens):\n",
    "        if token in sentence_tokens:\n",
    "            words_data[i, j] = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "method1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "U , S, V = svd(words_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 most important words:\n",
      "product\n",
      "featur\n",
      "new\n",
      "competit\n",
      "compani\n"
     ]
    }
   ],
   "source": [
    "words_and_values = [(word, U[idx, 0]) for idx, word in enumerate(all_tokens)]\n",
    "\n",
    "# Sort the list based on the absolute values of the matrix values\n",
    "sorted_words_and_values = sorted(words_and_values, key=lambda x: abs(x[1]), reverse=True)\n",
    "\n",
    "# Extract the top 5 words\n",
    "top_5_words = [word for word, _ in sorted_words_and_values[:5]]\n",
    "\n",
    "# Print the top 5 words\n",
    "print(\"Top 5 most important words:\")\n",
    "for word in top_5_words:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 most important sentences:\n",
      "The example illustrates\n",
      "the real business pressures on companies: the need for speed, the\n",
      "concern about costs, the competition that may force the company\n",
      "to change its offerings, and the need to satisfy several classes of\n",
      "customers—investors, distributors, and, of course, the people who\n",
      "will actually use the product.\n",
      "• A competing company adds new features to its products, producing\n",
      "competitive pressures to match that offering, but to do even more in\n",
      "order to get ahead of the competition.\n",
      "FEATURITIS: A DEADLY TEMPTATION\n",
      "In every successful product there lurks the carrier of an insidious\n",
      "disease called “featuritis,” with its main symptom being “creeping featurism.” The disease seems to have been first identified and\n",
      "named in 1976, but its origins probably go back to the earliest technologies, buried far back in the eons prior to the dawn of history.\n",
      "Even relatively stable home products, such as automobiles, kitchen appliances, television sets, and\n",
      "computers, face the multiple forces of a competitive market that\n",
      "encourage the introduction of changes without sufficient testing\n",
      "and refinement.\n",
      "The problem is that after the product has been available for a\n",
      "while, a number of factors inevitably appear, pushing the company\n",
      "toward the addition of new features—toward creeping featurism.\n"
     ]
    }
   ],
   "source": [
    "sentences_and_values = [(idx, V.T[idx, 0]) for idx in range(V.shape[1])]\n",
    "\n",
    "# Sort the list based on the absolute values of the matrix values\n",
    "sorted_sentences_and_values = sorted(sentences_and_values, key=lambda x: abs(x[1]), reverse=True)\n",
    "\n",
    "# Extract the top 5 sentences\n",
    "top_5_sentences_indices = [idx for idx, _ in sorted_sentences_and_values[:5]]\n",
    "\n",
    "# Print the top 5 sentences\n",
    "print(\"Top 5 most important sentences:\")\n",
    "for idx in top_5_sentences_indices:\n",
    "    print(sentences[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "method2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def get_permutation_matrix(col_1, col_2, size):\n",
    "    I = np.eye(size)\n",
    "    temt = I[:, col_1].copy()\n",
    "    I[:, col_1] = I[:, col_2].copy()\n",
    "    I[:, col_2] = temt\n",
    "    return I\n",
    "\n",
    "def get_norm(x):\n",
    "    return np.sqrt(np.sum(np.square(x)))\n",
    "\n",
    "def householder(X):\n",
    "    v = column_converter(X)\n",
    "    size_of_v = v.shape[1]\n",
    "    e1 = np.zeros_like(v)\n",
    "    e1[0, 0] = 1\n",
    "    vector = get_norm(v) * e1\n",
    "    if v[0, 0] < 0:\n",
    "        vector = -vector\n",
    "    u = (v + vector).astype(np.float32)\n",
    "    a = np.identity(size_of_v)\n",
    "    b1 = (2 * np.matmul(np.transpose(u), u))\n",
    "    b2 = np.matmul(u, np.transpose(u))\n",
    "    b = b1 / b2\n",
    "    H = a - b\n",
    "    H[H == -np.inf] = 0\n",
    "    return H\n",
    "\n",
    "def column_converter(x):\n",
    "    x.shape = (1, x.shape[0])\n",
    "    return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_columns = [f'sentence_{i}' for i in range(len(sentences))]\n",
    "sorted_sentence_columns = sentence_columns.copy()\n",
    "words_dataframe = pd.DataFrame(words_data, columns=sentence_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For k=5, the most important sentence is:\n",
      "Many companies claim to aspire to this philosophy, but few are able to follow\n",
      "it.\n",
      "\n",
      "\n",
      "For k=6, the most important sentence is:\n",
      "Many companies claim to aspire to this philosophy, but few are able to follow\n",
      "it.\n",
      "\n",
      "\n",
      "For k=7, the most important sentence is:\n",
      "Many companies claim to aspire to this philosophy, but few are able to follow\n",
      "it.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "U, S, V = np.linalg.svd(words_data, full_matrices=False)\n",
    "C, D = U, np.diag(S).dot(V)\n",
    "\n",
    "# List to store the top k important sentences for different values of k\n",
    "top_sentences_list = []\n",
    "\n",
    "# Iterate for k = 5, 6, 7\n",
    "for k_value in range(5, 8):\n",
    "    P_list = []\n",
    "    \n",
    "    for i in range(k_value):\n",
    "        arr = D[:, i:].copy()\n",
    "        col_sum = np.apply_along_axis(np.linalg.norm, axis=0, arr=arr)\n",
    "        max_col = np.argmax(col_sum)\n",
    "        P = get_permutation_matrix(i, max_col + i, D.shape[1])\n",
    "        sorted_sentence_columns[i], sorted_sentence_columns[max_col + i] = sorted_sentence_columns[max_col + i], sorted_sentence_columns[i]\n",
    "        D = D.dot(P).copy()\n",
    "        Q = householder(D[:, i].reshape((-1, 1)))\n",
    "        D_a = np.round(Q.dot(D[:, i:]), 12)\n",
    "        D[:, i:] = D_a.copy()\n",
    "        P_list.append(P)\n",
    "    \n",
    "    words_dataframe_sorted = words_dataframe.copy()\n",
    "\n",
    "    for P in P_list:\n",
    "        words_dataframe_sorted = words_dataframe_sorted @ P\n",
    "\n",
    "    words_dataframe_sorted.columns = sorted_sentence_columns\n",
    "    # Get the most important sentence for the current k\n",
    "    most_important_sentence = words_dataframe_sorted.idxmax(axis=1)[0]\n",
    "    top_sentences_list.append((k_value, most_important_sentence))\n",
    "\n",
    "# Print the results\n",
    "for k_value, most_important_sentence in top_sentences_list:\n",
    "    print(f\"For k={k_value}, the most important sentence is:\")\n",
    "    sentence_index = int(most_important_sentence.split('_')[1])  # Extract the index from the column name\n",
    "    print(sentences[sentence_index])\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
